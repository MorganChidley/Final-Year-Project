{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsjxYBmMthZNjU31Mlhvf+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorganChidley/Final-Year-Project/blob/main/ML_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ3Ye1r9bkHy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"MyDataSET.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the data\n",
        "data.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"MyDataSET.csv\"\n",
        "data= pd.read_csv(file_path)\n",
        "\n",
        "# Get the shape of the data (number of rows and columns)\n",
        "data.shape"
      ],
      "metadata": {
        "id": "cpYLf_EHq8YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"MyDataSET.csv\"\n",
        "data= pd.read_csv(file_path)\n",
        "\n",
        "# Check for missing values\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "gV_JXeZArJG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"MyDataSET.csv\"\n",
        "data= pd.read_csv(file_path)\n",
        "\n",
        "# Remove duplicate rows\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Check the new shape of the dataset\n",
        "data.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "CeT-QjqGra0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Feature extraction functions\n",
        "\n",
        "def url_length(url):\n",
        "    \"\"\"Returns the length of the URL.\"\"\"\n",
        "    return len(url)\n",
        "\n",
        "def has_ip_address(url):\n",
        "    \"\"\"Checks if the URL contains an IP address.\"\"\"\n",
        "    ip_pattern = r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"\n",
        "    return 1 if re.search(ip_pattern, url) else 0\n",
        "\n",
        "def count_special_chars(url):\n",
        "    \"\"\"Counts occurrences of special characters.\"\"\"\n",
        "    special_chars = [\"-\", \"_\", \"%\", \"/\", \".\", \"#\"]\n",
        "    return {char: url.count(char) for char in special_chars}\n",
        "\n",
        "def count_subdomains(url):\n",
        "    \"\"\"Counts the number of subdomains.\"\"\"\n",
        "    parsed_url = urlparse(url).netloc\n",
        "    return parsed_url.count('.')\n",
        "\n",
        "def has_https(url):\n",
        "    \"\"\"Checks if the URL uses HTTPS.\"\"\"\n",
        "    return 1 if urlparse(url).scheme == \"https\" else 0\n",
        "\n",
        "\n",
        "def count_query_parameters(url):\n",
        "    \"\"\"Counts the number of query parameters in the URL.\"\"\"\n",
        "    query = urlparse(url).query\n",
        "    return len(query.split(\"&\")) if query else 0\n",
        "\n",
        "# Apply feature extraction\n",
        "\n",
        "data[\"url_length\"] = data[\"URL\"].apply(url_length)\n",
        "data[\"subdomain_count\"] = data[\"URL\"].apply(count_subdomains)\n",
        "data[\"https\"] = data[\"URL\"].apply(has_https)\n",
        "data[\"has_ip_address\"] = data[\"URL\"].apply(has_ip_address)\n",
        "data[\"query_parameters_count\"] = data[\"URL\"].apply(count_query_parameters)\n",
        "\n",
        "# Extract special character counts only once\n",
        "if not all(char in data.columns for char in [\"-\", \"_\", \"%\", \"/\", \".\", \"#\"]):\n",
        "    special_chars_df = data[\"URL\"].apply(count_special_chars).apply(pd.Series)\n",
        "    data = pd.concat([data, special_chars_df], axis=1)\n",
        "\n",
        "# Remove duplicate columns\n",
        "data = data.loc[:, ~data.columns.duplicated()]\n",
        "\n",
        "# Converts all feature changes to a new csv file\n",
        "data.to_csv(\"modified_dataset.csv\", index=False)\n",
        "\n",
        "# Display first few rows with new features\n",
        "data.head()"
      ],
      "metadata": {
        "id": "FbqSXhZ37UZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from IPython.display import display\n",
        "\n",
        "# Load Dataset\n",
        "data = pd.read_csv(\"modified_dataset.csv\")\n",
        "\n",
        "# Feature Selection\n",
        "features = ['url_length', 'subdomain_count', 'https', \"-\", \"_\", \"%\", \"/\", \".\", \"#\"]\n",
        "X = data[features]\n",
        "y = data['ClassLabel']  # Target variable\n",
        "\n",
        "# Handle missing values (fill with median)\n",
        "X.fillna(X.median(), inplace=True)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate multiple models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
        "    \"SVM\": SVC(),\n",
        "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)\n",
        "}\n",
        "\n",
        "# Store results in a list of dictionaries\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "\n",
        "   # Extract precision, recall, and F1-score for each class\n",
        "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "    precision = report_dict['weighted avg']['precision']\n",
        "    recall = report_dict['weighted avg']['recall']\n",
        "    f1_score = report_dict['weighted avg']['f1-score']\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1_score,\n",
        "        'Confusion Matrix': conf_matrix,\n",
        "    })\n",
        "\n",
        "# Create a Pandas DataFrame from the results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "AoeAkr3I-jKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Load Dataset\n",
        "data = pd.read_csv(\"modified_dataset.csv\")\n",
        "\n",
        "# Drop the 'ClassLabel' column\n",
        "data = data.drop('ClassLabel', axis=1)\n",
        "\n",
        "data.hist(figsize=(15, 30))  # Adjust figsize as needed\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jBFIdU2YYEgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Load Dataset\n",
        "data = pd.read_csv(\"modified_dataset.csv\")\n",
        "\n",
        "# Drop the 'ClassLabel' column\n",
        "data = data.drop('ClassLabel', axis=1)\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_data = data.select_dtypes(include=np.number)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = numeric_data.corr()\n",
        "\n",
        "# Display correlation matrix as a styled table\n",
        "correlation_table = correlation_matrix.style.background_gradient(cmap='coolwarm')\n",
        "\n",
        "correlation_table"
      ],
      "metadata": {
        "id": "a21Wh-36epOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from IPython.display import display # Import for display function\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"modified_dataset.csv\")\n",
        "\n",
        "X = data.drop(['ClassLabel', \"URL\"], axis=1)\n",
        "y = data['ClassLabel']\n",
        "\n",
        "# Mutual Information\n",
        "mutual_info = mutual_info_classif(X, y)\n",
        "feature_scores = pd.DataFrame({'Feature': X.columns, 'Mutual_Information': mutual_info})\n",
        "feature_scores = feature_scores.sort_values(by=['Mutual_Information'], ascending=False)\n",
        "\n",
        "\n",
        "# Display mutual information scores in a table\n",
        "display(feature_scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "UMXqhEcfehhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qw3UKYXTcV7y"
      }
    }
  ]
}